{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This Pytorch tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) shows how to train an LSTM for predicting a part-of-speech tag (noun, verb, etc) for words in a sentence. The inputs are words and the outputs are tags.\n",
    "\n",
    "Below is a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the sentence and the tags are first encoded as integers:  \n",
    "\n",
    "- inputs: `\"The dog ate the apple\"` --> `[\"The\", \"dog\", \"ate\", \"the\", \"apple\"]` --> `[0, 1, 2, 3, 4]`\n",
    "- targets: `[\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]` --> `[0, 1, 2, 0, 1]`\n",
    "\n",
    "The number of unique encoded words gives the `vocab_size` and the unique encoded tags give the `target_size`.\n",
    "\n",
    "The LSTM model gives a tag to a word as follows: \n",
    "\n",
    "1. word as a string\n",
    "2. encode word as integer in `[0, vocab_size]` \n",
    "3. embed to `embedding_dim` real numbers \n",
    "4. lstm hidden state: `hidden_dim` real numbers \n",
    "5. linear layer: `tagset_size` real numbers\n",
    "6. scores: `tagset_size` log-probabilities\n",
    "7. tag = argmax(scores)\n",
    "\n",
    "In practice, the above pipeline takes in a sentence of `sequence_length` words. All operations are vectorized over the words, except the LSTM layer that has a recursive operation (reduce?): information about each word is passed through the hidden state to the next word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Convert a word or tag to its index in the integer encoding.\"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # like onehot encode but to real numbers\n",
    "        # In: integers in [0, vocab_size)\n",
    "        # Out: numbers in R^embedding_dim.\n",
    "        # These are `vocab_size * embedding_dim` learnable parameters,\n",
    "        # they are inside model.parameters() which is passed to the optimizer.\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Word embeddings to hidden states\n",
    "        # In: R^embedding_dim,\n",
    "        # Out: R^hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Hidden states to tag space\n",
    "        # In: R^hidden_dim\n",
    "        # Out: R^tagset_size\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence_in):\n",
    "        embeds = self.word_embeddings(sentence_in)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence_in), 1, -1))\n",
    "        tag_space = self.hidden2tag(\n",
    "            lstm_out.view(len(sentence_in), -1)\n",
    "        )  # tagset_size numbers\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)  # log-probabilities\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    embedding_dim=4,\n",
    "    hidden_dim=6,\n",
    "    vocab_size=len(word_to_ix),\n",
    "    tagset_size=len(tag_to_ix),\n",
    ")\n",
    "loss_function = nn.NLLLoss()\n",
    "sentence, tags = training_data[0]\n",
    "sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "embeds = model.word_embeddings(sentence_in)\n",
    "lstm_out = model.lstm(embeds)[0]\n",
    "tag_space = model.hidden2tag(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Words and tags\n",
      " ['The', 'dog', 'ate', 'the', 'apple'] ['DET', 'NN', 'V', 'DET', 'NN']\n",
      "2. Encoded words and tags\n",
      " tensor([0, 1, 2, 3, 4]) tensor([0, 1, 2, 0, 1])\n",
      "3. Embedding\n",
      " tensor([[-0.7992, -0.8164, -0.1888, -1.1254],\n",
      "        [ 0.0066,  3.7635, -1.4803, -1.3627],\n",
      "        [-1.9548,  0.6337,  1.7014,  1.2115],\n",
      "        [ 0.5464, -1.6631,  0.1975, -2.0238],\n",
      "        [-0.0664,  0.5113,  1.8811, -0.5149]], grad_fn=<EmbeddingBackward0>)\n",
      "4. LSTM\n",
      " tensor([[-0.1046,  0.0785,  0.0024, -0.0414, -0.1322,  0.0199],\n",
      "        [ 0.0335, -0.4348,  0.0360, -0.2240, -0.3378, -0.3356],\n",
      "        [ 0.2729, -0.4089, -0.0230, -0.3448, -0.1964, -0.1101],\n",
      "        [ 0.0023, -0.0265,  0.0568, -0.0335, -0.2466,  0.0274],\n",
      "        [ 0.0989, -0.3172,  0.0404, -0.1156, -0.1845,  0.1310]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "5. Linear layer\n",
      " tensor([[-0.0543,  0.3550,  0.0950],\n",
      "        [-0.0334,  0.4461,  0.0407],\n",
      "        [ 0.1042,  0.4463, -0.0926],\n",
      "        [-0.0433,  0.3911,  0.0404],\n",
      "        [ 0.1101,  0.4131, -0.0273]], grad_fn=<AddmmBackward0>)\n",
      "6. Scores\n",
      " tensor([[0.2727, 0.4106, 0.3166],\n",
      "        [0.2708, 0.4375, 0.2917],\n",
      "        [0.3097, 0.4360, 0.2544],\n",
      "        [0.2754, 0.4252, 0.2994],\n",
      "        [0.3100, 0.4197, 0.2702]], grad_fn=<SoftmaxBackward0>)\n",
      "7. argmax\n",
      " tensor([1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Words and tags\\n\", sentence, tags)\n",
    "print(\"2. Encoded words and tags\\n\", sentence_in, targets)\n",
    "print(\"3. Embedding\\n\", embeds)\n",
    "print(\"4. LSTM\\n\", lstm_out)\n",
    "print(\"5. Linear layer\\n\", tag_space)\n",
    "print(\"6. Scores\\n\", F.softmax(tag_space, dim=1))\n",
    "print(\"7. argmax\\n\", torch.argmax(F.softmax(tag_space, dim=1), axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
